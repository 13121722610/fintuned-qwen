import torch
import os

# ========== è®¾ç½®é•œåƒæº ==========
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['CUDA_VISIBLE_DEVICES'] = '4'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'

print("âœ… ä½¿ç”¨HFé•œåƒ: https://hf-mirror.com")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
from datetime import datetime
import sys

class MedicalChatBot:
    """åŒ»ç–—èŠå¤©æœºå™¨äºº"""
    
    def __init__(self, lora_path=None):
        print("=" * 70)
        print("ğŸ¥ åŒ»ç–—åŠ©æ‰‹èŠå¤©æœºå™¨äºº")
        print("=" * 70)
        
        # åŸºç¡€æ¨¡å‹
        self.base_model = "Qwen/Qwen2.5-7B-Instruct"
        
        # LoRAé€‚é…å™¨è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„ï¼‰
        if lora_path is None:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç›®å½•
            import glob
            lora_dirs = glob.glob("/amax/home/yhji/LM-Course/finetuned_model_*")
            if lora_dirs:
                lora_dirs.sort(key=os.path.getmtime, reverse=True)
                self.lora_path = lora_dirs[0]
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æœ€æ–°æ¨¡å‹: {os.path.basename(self.lora_path)}")
            else:
                self.lora_path = "/amax/home/yhji/LM-Course/finetuned_model_20251206_133554"
                print(f"âš ï¸  ä½¿ç”¨é»˜è®¤æ¨¡å‹: {self.lora_path}")
        else:
            self.lora_path = lora_path
        
        print(f"ğŸ“¥ åŸºç¡€æ¨¡å‹: {self.base_model}")
        print(f"ğŸ“¥ LoRAé€‚é…å™¨: {self.lora_path}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()
        
        # å†å²è®°å½•
        self.conversation_history = []
        self.save_dir = "/amax/home/yhji/LM-Course/chat_history"
        os.makedirs(self.save_dir, exist_ok=True)
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("\nâ³ åŠ è½½æ¨¡å‹ä¸­...")
        
        try:
            # 1. åŠ è½½tokenizerï¼ˆä¼˜å…ˆä½¿ç”¨è®­ç»ƒç›®å½•ä¸­çš„ï¼‰
            tokenizer_path = os.path.join(self.lora_path, "tokenizer")
            if os.path.exists(tokenizer_path):
                self.tokenizer = AutoTokenizer.from_pretrained(
                    tokenizer_path,
                    trust_remote_code=True
                )
                print("âœ… ä½¿ç”¨è®­ç»ƒç›®å½•ä¸­çš„tokenizer")
            else:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.base_model,
                    trust_remote_code=True
                )
                print("âœ… ä»HFåŠ è½½tokenizer")
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # 2. åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹...")
            self.base_model_inst = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # 3. åŠ è½½LoRAæƒé‡
            print("ğŸ¯ åŠ è½½LoRAé€‚é…å™¨...")
            self.model = PeftModel.from_pretrained(
                self.base_model_inst, 
                self.lora_path
            )
            self.model.eval()
            
            print("ğŸš€ æ¨¡å‹åŠ è½½å®Œæˆï¼")
            print(f"ğŸ® è®¾å¤‡: {self.model.device}")
            
            # æµ‹è¯•ä¸€ä¸ªç®€å•é—®é¢˜
            test_response = self._generate_response("ä½ å¥½")
            if test_response:
                print("âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡")
            else:
                print("âš ï¸  æ¨¡å‹å“åº”å¼‚å¸¸")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def _generate_response(self, user_input, temperature=0.7, max_tokens=500):
        """ç”Ÿæˆå›ç­”"""
        # æ„å»ºå¯¹è¯
        user_content = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„ç—‡çŠ¶æè¿°ç»™å‡ºä¸“ä¸šã€æ¸…æ™°ä¸”å®ç”¨çš„å¥åº·å»ºè®®ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{user_input}"
        
        messages = [
            {"role": "user", "content": user_content},
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        try:
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            # å¦‚æœæ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
            text = f"<|im_start|>user\n{user_content}<|im_end|>\n<|im_start|>assistant\n"
        
        # Tokenize
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç 
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–åŠ©æ‰‹å›ç­”
        if "assistant" in response:
            answer = response.split("assistant")[-1].strip()
        elif user_content in response:
            answer = response.split(user_content)[-1].strip()
        else:
            answer = response
        
        return answer.strip()
    
    def chat(self):
        """å¼€å§‹èŠå¤©"""
        print("\n" + "="*70)
        print("ğŸ’¬ å¼€å§‹èŠå¤© (è¾“å…¥ 'quit' é€€å‡º, 'help' æŸ¥çœ‹å‘½ä»¤)")
        print("="*70)
        
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_history = []
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†å‘½ä»¤
                if user_input.lower() == 'quit':
                    print("ğŸ‘‹ å†è§ï¼")
                    self._save_session(session_id, session_history)
                    break
                
                elif user_input.lower() == 'help':
                    self._show_help()
                    continue
                
                elif user_input.lower() == 'history':
                    self._show_history(session_history)
                    continue
                
                elif user_input.lower() == 'clear':
                    session_history = []
                    print("ğŸ§¹ å†å²è®°å½•å·²æ¸…é™¤")
                    continue
                
                elif user_input.lower() == 'save':
                    self._save_session(session_id, session_history)
                    continue
                
                elif user_input.lower() == 'params':
                    self._adjust_parameters()
                    continue
                
                # ç”Ÿæˆå›ç­”
                print("ğŸ¤– æ€è€ƒä¸­...", end="", flush=True)
                
                response = self._generate_response(user_input)
                
                # æ¸…ç©º"æ€è€ƒä¸­"
                print("\r" + " " * 20 + "\r", end="", flush=True)
                
                # æ‰“å°å›ç­”
                print(f"ğŸ¥ åŠ©æ‰‹: {response}")
                
                # ä¿å­˜åˆ°å†å²
                session_history.append({
                    "user": user_input,
                    "assistant": response,
                    "timestamp": datetime.now().isoformat()
                })
                
                # è‡ªåŠ¨ä¿å­˜ï¼ˆæ¯5è½®ï¼‰
                if len(session_history) % 5 == 0:
                    self._save_session(session_id, session_history)
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸  ä¸­æ–­æ“ä½œ")
                save = input("æ˜¯å¦ä¿å­˜èŠå¤©è®°å½•ï¼Ÿ(y/n): ")
                if save.lower() == 'y':
                    self._save_session(session_id, session_history)
                break
                
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
                continue
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©"""
        print("\nğŸ“‹ å¯ç”¨å‘½ä»¤:")
        print("  help     - æ˜¾ç¤ºæ­¤å¸®åŠ©")
        print("  history  - æŸ¥çœ‹å½“å‰ä¼šè¯å†å²")
        print("  clear    - æ¸…é™¤å½“å‰ä¼šè¯å†å²")
        print("  save     - ä¿å­˜å½“å‰ä¼šè¯")
        print("  params   - è°ƒæ•´ç”Ÿæˆå‚æ•°")
        print("  quit     - é€€å‡ºèŠå¤©")
    
    def _show_history(self, history):
        """æ˜¾ç¤ºå†å²"""
        if not history:
            print("ğŸ“ å½“å‰æ²¡æœ‰èŠå¤©å†å²")
            return
        
        print("\nğŸ“œ èŠå¤©å†å²:")
        for i, item in enumerate(history, 1):
            print(f"\n--- ç¬¬{i}è½® ---")
            print(f"ğŸ‘¤ æ‚¨: {item['user']}")
            print(f"ğŸ¥ åŠ©æ‰‹: {item['assistant'][:100]}...")
    
    def _save_session(self, session_id, history):
        """ä¿å­˜ä¼šè¯"""
        if not history:
            print("ğŸ“ æ²¡æœ‰å†…å®¹å¯ä¿å­˜")
            return
        
        filename = f"{self.save_dir}/chat_{session_id}.json"
        data = {
            "model": "Qwen2.5-7B-Medical-Finetuned",
            "lora_path": self.lora_path,
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "history": history
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ èŠå¤©è®°å½•å·²ä¿å­˜: {filename}")
    
    def _adjust_parameters(self):
        """è°ƒæ•´ç”Ÿæˆå‚æ•°"""
        print("\nâš™ï¸  è°ƒæ•´ç”Ÿæˆå‚æ•°:")
        try:
            temp = float(input("æ¸©åº¦ (0.1-1.0, å½“å‰0.7): ") or 0.7)
            tokens = int(input("æœ€å¤§ç”Ÿæˆé•¿åº¦ (100-1000, å½“å‰500): ") or 500)
            
            # éªŒè¯èŒƒå›´
            temp = max(0.1, min(1.0, temp))
            tokens = max(100, min(1000, tokens))
            
            # è¿™é‡Œå¯ä»¥ä¿å­˜å‚æ•°ï¼Œç®€åŒ–ç‰ˆæœ¬å…ˆä¸å®ç°
            print(f"âœ… å‚æ•°å·²æ›´æ–°: æ¸©åº¦={temp}, æœ€å¤§é•¿åº¦={tokens}")
        except:
            print("âš ï¸  å‚æ•°è°ƒæ•´å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")


# ========== ä»¥ä¸‹æ˜¯æ–°å¢çš„Web APIéƒ¨åˆ† ==========

class MedicalChatAPI:
    """ä¸ºWebåº”ç”¨å°è£…çš„èŠå¤©æœºå™¨äººAPI"""
    
    def __init__(self, model_path=None):
        """
        åˆå§‹åŒ–API
        
        Args:
            model_path: LoRAæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        """
        print("ğŸš€ åˆå§‹åŒ–Web API...")
        self.bot = MedicalChatBot(lora_path=model_path)
        print("âœ… Web APIåˆå§‹åŒ–å®Œæˆ")
    
    def chat(self, message, temperature=0.7, max_tokens=500):
        """
        å•æ¬¡å¯¹è¯æ¥å£
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            str: åŠ©æ‰‹å›å¤
        """
        # æ„å»ºå®Œæ•´çš„å¯¹è¯æ ¼å¼
        full_prompt = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„ç—‡çŠ¶æè¿°ç»™å‡ºä¸“ä¸šã€æ¸…æ™°ä¸”å®ç”¨çš„å¥åº·å»ºè®®ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{message}"
        
        # è°ƒç”¨åŸæœ‰æ¨¡å‹çš„ç”Ÿæˆæ–¹æ³•
        response = self.bot._generate_response(
            full_prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # æ¸…ç†å›å¤æ ¼å¼
        cleaned_response = self._clean_response(response)
        return cleaned_response
    
    def _clean_response(self, response):
        """æ¸…ç†å›å¤ï¼Œç§»é™¤å¤šä½™çš„æ ¼å¼æ ‡è®°"""
        # ç§»é™¤å¯èƒ½çš„æ¨¡æ¿æ ‡è®°
        markers = ["<|im_start|>", "<|im_end|>", "assistant", "user", "system"]
        for marker in markers:
            response = response.replace(marker, "")
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "base_model": self.bot.base_model,
            "lora_path": self.bot.lora_path,
            "device": str(self.bot.model.device) if hasattr(self.bot, 'model') else "unknown"
        }


def test_api():
    """æµ‹è¯•APIæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("ğŸ§ª æµ‹è¯•MedicalChatAPI...")
    
    try:
        # åˆ›å»ºAPIå®ä¾‹
        api = MedicalChatAPI()
        
        # æµ‹è¯•ç®€å•é—®é¢˜
        test_questions = [
            "æˆ‘å¤´ç—›æ€ä¹ˆåŠï¼Ÿ",
            "æ„Ÿå†’äº†åº”è¯¥åƒä»€ä¹ˆè¯ï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\nâ“ é—®é¢˜: {question}")
            response = api.chat(question)
            print(f"ğŸ’Š å›ç­”: {response[:100]}...")
        
        print("\nâœ… APIæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åŒ»ç–—åŠ©æ‰‹èŠå¤©æœºå™¨äºº")
    parser.add_argument("--model", type=str, help="LoRAæ¨¡å‹è·¯å¾„", default=None)
    parser.add_argument("--test", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--test-api", action="store_true", help="æµ‹è¯•Web API")
    
    args = parser.parse_args()
    
    # æµ‹è¯•APIæ¨¡å¼
    if args.test_api:
        test_api()
        return
    
    # åˆ›å»ºèŠå¤©æœºå™¨äºº
    bot = MedicalChatBot(lora_path=args.model)
    
    # æµ‹è¯•æ¨¡å¼
    if args.test:
        print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•æ¨¡å¼:")
        test_questions = [
            "æˆ‘å¤´ç—›æ€ä¹ˆåŠï¼Ÿ",
            "æ„Ÿå†’äº†åº”è¯¥åƒä»€ä¹ˆè¯ï¼Ÿ",
            "é«˜è¡€å‹è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"
        ]
        
        for q in test_questions:
            print(f"\nâ“ æµ‹è¯•é—®é¢˜: {q}")
            response = bot._generate_response(q)
            print(f"ğŸ’Š å›ç­”: {response[:150]}...")
        
        print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ")
        return
    
    # äº¤äº’æ¨¡å¼
    bot.chat()

if __name__ == "__main__":
    main()
    