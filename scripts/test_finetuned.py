import torch
import os

# ========== è®¾ç½®é•œåƒæºï¼ˆæœ€é‡è¦ï¼ï¼‰ ==========
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

print("âœ… ä½¿ç”¨HFé•œåƒ: https://hf-mirror.com")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
from datetime import datetime

# è®¾ç½®GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def test_finetuned_model():
    """æµ‹è¯•å¾®è°ƒåçš„åŒ»ç–—æ¨¡å‹"""
    
    print("=" * 70)
    print("ğŸ§ª æµ‹è¯•å¾®è°ƒåçš„Qwen2.5-7BåŒ»ç–—æ¨¡å‹")
    print("=" * 70)
    
    # åŸºç¡€æ¨¡å‹è·¯å¾„
    base_model = "Qwen/Qwen2.5-7B-Instruct"
    
    # LoRAé€‚é…å™¨è·¯å¾„ï¼ˆä½ çš„è®­ç»ƒè¾“å‡ºç›®å½•ï¼‰
    lora_path = "/amax/home/yhji/LM-Course/finetuned_model_20251212_090808"
    
    # è¾“å‡ºç›®å½•
    output_dir = "/amax/home/yhji/LM-Course/output"
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")
    print(f"ğŸ“¥ åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
    
    # 1. åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model,
        trust_remote_code=True
    )
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    print("\nğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model_inst = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # 3. åŠ è½½LoRAæƒé‡
    print("ğŸ¯ åŠ è½½LoRAé€‚é…å™¨...")
    model = PeftModel.from_pretrained(base_model_inst, lora_path)
    model.eval()
    
    # 4. æµ‹è¯•é—®é¢˜ï¼ˆä¸è®­ç»ƒæ•°æ®ç›¸åŒæ ¼å¼ï¼‰
    test_questions = [
        "æ’åµæ—¥åŒæˆ¿è¿‡åä¸€ç›´å°è…¹ç—›è…°ç—›æ€ä¹ˆå›äº‹ç¦»æœˆç»æœŸé—´è¿˜æœ‰ä¹å¤©è¯·é—®åŒ»ç”Ÿæˆ‘è¿™æ˜¯æ€ä¹ˆäº†",
        "æœ‰åšç£å…±æŒ¯è¯·åŒ»ç”Ÿå»ºè®®ï¼Œç°åœ¨æƒ³é—®ä¸‹å¾—æ€ä¹ˆæ²»ç–—ï¼Œéœ€è¦ç”¨ä»€ä¹ˆè¯ç‰©è¿˜æ˜¯ä½é™¢æ²»ç–—åŠ¨æ‰‹æœ¯", 
        "å¤§çº¦å¿«æœ‰ä¸€å¹´å¤šäº†ï¼Œæ—©ä¸Šèµ·æ¥å˜´é‡Œå¥½å¤šå£æ°´ï¼Œç°åœ¨æ—©æ™šåˆ·ç‰™ï¼Œç‰™é½¿ä¹Ÿæ²¡æœ‰ç•¸å½¢ä»€ä¹ˆçš„ï¼Œåˆ°åº•æ˜¯ä»€ä¹ˆåŸå› å•Šï¼Ÿ",
    ]
    
    results = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“ æµ‹è¯• {i}/{len(test_questions)}")
        print(f"ğŸ’¬ é—®é¢˜: {question}")
        
        # æ„å»ºå¯¹è¯ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ ¼å¼ï¼‰
        user_content = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„ç—‡çŠ¶æè¿°ç»™å‡ºä¸“ä¸šã€æ¸…æ™°ä¸”å®ç”¨çš„å¥åº·å»ºè®®ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š{question}"
        
        messages = [
            {"role": "user", "content": user_content},
        ]
        
        # åº”ç”¨Qwençš„èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Tokenize
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆå›ç­”
        print("â³ ç”Ÿæˆå›ç­”...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=500,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç 
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–åŠ©æ‰‹å›ç­”
        if "assistant" in response:
            answer = response.split("assistant")[-1].strip()
        elif user_content in response:
            answer = response.split(user_content)[-1].strip()
        else:
            answer = response
        
        print(f"ğŸ’Š å›ç­”: {answer}...")
        
        results.append({
            "id": i,
            "question": question,
            "answer": answer,
            "timestamp": datetime.now().isoformat()
        })
    
    # 5. ä¿å­˜ç»“æœ
    output_file = f"{output_dir}/finetuned_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "model": "Qwen2.5-7B-Instruct-Finetuned-Medical",
            "base_model": base_model,
            "lora_path": lora_path,
            "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š å…±æµ‹è¯• {len(results)} ä¸ªé—®é¢˜")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {output_file}")
    
    # 6. æ˜¾ç¤ºå¯¹æ¯”ï¼ˆå¯ä»¥å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåæ¨¡å‹ï¼‰
    print(f"\nğŸ“‹ å¿«é€ŸæŸ¥çœ‹ç»“æœ:")
    for i, result in enumerate(results[:3], 1):
        print(f"\n--- é—®é¢˜ {i} ---")
        print(f"â“: {result['question']}...")
        print(f"ğŸ’Š: {result['answer']}...")

if __name__ == "__main__":
    test_finetuned_model()
