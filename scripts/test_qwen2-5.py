import torch
import os

# ========== è®¾ç½®é•œåƒæºï¼ˆæœ€é‡è¦ï¼ï¼‰ ==========
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

print("âœ… ä½¿ç”¨HFé•œåƒ: https://hf-mirror.com")

from transformers import AutoModelForCausalLM, AutoTokenizer
import json
from datetime import datetime

def main():
    MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
    
    # 1. è®¾ç½®è¾“å‡ºç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    OUTPUT_DIR = "/amax/home/yhji/LM-Course/output"
    OUTPUT_FILE = os.path.join(OUTPUT_DIR, "qwen_baseline_responses.json")
    
    # 2. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•: {MODEL_NAME}")
    print(f"ğŸ® å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    
    # 3. æµ‹è¯•é—®é¢˜
    test_questions = [
        "æ’åµæ—¥åŒæˆ¿è¿‡åä¸€ç›´å°è…¹ç—›è…°ç—›æ€ä¹ˆå›äº‹ç¦»æœˆç»æœŸé—´è¿˜æœ‰ä¹å¤©è¯·é—®åŒ»ç”Ÿæˆ‘è¿™æ˜¯æ€ä¹ˆäº†",
        "ä½ å¥½ï¼Œå…¨èº«æ²¡åŠ²ï¼Œæ²¡ç²¾ç¥ï¼Œåƒä¸ä¸‹é¥­ï¼Œåªæƒ³ç¡è§‰ï¼Œæ˜¯ä»€ä¹ˆæƒ…å†µ",
        "ç°å¹´34å²ï¼ŒåŒ»ç”Ÿè¯Šæ–­æ˜¯çœ¼ç›é‡Œè¡€ç®¡å µå¡ï¼Œè¯·é—®æ€æ ·èƒ½æ²»å¥½è°¢è°¢",
    ]
    
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    try:
        # 4. åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        # 5. åŠ è½½æ¨¡å‹ - ä½¿ç”¨å¤šGPU
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        if hasattr(model, 'hf_device_map'):
            print(f"ğŸ“Š æ¨¡å‹åˆ†å¸ƒåœ¨ä»¥ä¸‹è®¾å¤‡: {model.hf_device_map}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨CPUæ¨¡å¼...")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨CPU
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆCPUæ¨¡å¼ï¼‰")
    
    model.eval()
    
    # 6. ç”Ÿæˆå›ç­”
    results = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“ æµ‹è¯• {i}/{len(test_questions)}: {question}")
        
        # æ„å»ºå¯¹è¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": question}
        ]
        
        # æ ¼å¼åŒ–
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ç¼–ç 
        device = model.device if hasattr(model, 'device') else 'cpu'
        inputs = tokenizer(text, return_tensors="pt").to(device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=350,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç 
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–åŠ©æ‰‹å›ç­”
        if "assistant" in response:
            answer = response.split("assistant")[-1].strip()
        else:
            # å¤‡é€‰æå–æ–¹æ³•
            answer = response.split(question)[-1].strip() if question in response else response
        
        print(f"ğŸ’¬ æ¨¡å‹å›ç­”: {answer}")
        
        results.append({
            "id": i,
            "question": question,
            "response": answer,
            "model": MODEL_NAME,
            "timestamp": datetime.now().isoformat(),
            "device": str(device)
        })
    
    # 7. ä¿å­˜ç»“æœ
    output_data = {
        "model": MODEL_NAME,
        "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        "gpu_info": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [],
        "results": results
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š å…±æµ‹è¯• {len(results)} ä¸ªé—®é¢˜")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {os.path.basename(OUTPUT_FILE)}")
    
    # 8. æ˜¾ç¤ºç›®å½•å†…å®¹
    print(f"\nğŸ“‚ è¾“å‡ºç›®å½•å†…å®¹:")
    try:
        files = os.listdir(OUTPUT_DIR)
        for file in files:
            file_path = os.path.join(OUTPUT_DIR, file)
            if os.path.isfile(file_path):
                size = os.path.getsize(file_path) / 1024  # KB
                print(f"  {file} ({size:.1f} KB)")
    except Exception as e:
        print(f"  æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {e}")
    
    # 9. æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        print(f"\nğŸ® GPUä½¿ç”¨ç»Ÿè®¡:")
        for i in range(torch.cuda.device_count()):
            mem_used = torch.cuda.memory_allocated(i) / 1024**3
            mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {mem_used:.1f}/{mem_total:.1f} GB ({mem_used/mem_total*100:.1f}%)")

if __name__ == "__main__":
    main()
    