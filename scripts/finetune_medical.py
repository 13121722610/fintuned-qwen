import torch
import os

# ========== ä¿®å¤OpenBLASè­¦å‘Š ==========
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
# =====================================

# æŒ‡å®šä½¿ç”¨GPU 0
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# è®¾ç½®é•œåƒæº
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import json
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    TrainerCallback,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_from_disk
import numpy as np
import warnings
warnings.filterwarnings('ignore')

print("âœ… ä½¿ç”¨HFé•œåƒ: https://hf-mirror.com")

class TrainingPlotter:
    """è®­ç»ƒè¿‡ç¨‹å›¾è¡¨ç”Ÿæˆå™¨"""
    def __init__(self, output_dir):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        self.train_losses = []          # è®­ç»ƒloss
        self.eval_losses = []           # éªŒè¯loss
        self.learning_rates = []        # å­¦ä¹ ç‡
        self.train_steps = []           # è®­ç»ƒæ­¥æ•°
        self.eval_steps = []            # éªŒè¯æ­¥æ•°
        self.logs_history = []          # å®Œæ•´æ—¥å¿—å†å²
        self.epoch_logs = []            # epochçº§åˆ«æ—¥å¿—
        
    def add_log(self, log_dict, step=None, epoch=None):
        """æ·»åŠ æ—¥å¿—"""
        log_dict_copy = log_dict.copy()
        
        # æ·»åŠ æ—¶é—´æˆ³
        log_dict_copy['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if step is not None:
            log_dict_copy['step'] = step
        if epoch is not None:
            log_dict_copy['epoch'] = epoch
        
        self.logs_history.append(log_dict_copy)
        
        # æå–è®­ç»ƒloss
        if 'loss' in log_dict and log_dict['loss'] is not None:
            current_step = len(self.train_losses) + 1
            self.train_losses.append(log_dict['loss'])
            self.train_steps.append(current_step)
            
            # æ¯50æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒè¿›åº¦
            if current_step % 50 == 0:
                print(f"ğŸ“Š è®­ç»ƒè¿›åº¦: Step {current_step}, Loss: {log_dict['loss']:.4f}")
        
        # æå–éªŒè¯loss
        if 'eval_loss' in log_dict and log_dict['eval_loss'] is not None:
            current_eval_step = len(self.eval_losses) + 1
            self.eval_losses.append(log_dict['eval_loss'])
            self.eval_steps.append(current_eval_step)
            
            print(f"ğŸ“ˆ Epoch {log_dict.get('epoch', '?')} éªŒè¯Loss: {log_dict['eval_loss']:.4f}")
        
        # æå–å­¦ä¹ ç‡
        if 'learning_rate' in log_dict:
            self.learning_rates.append(log_dict['learning_rate'])
        
        # æå–epochä¿¡æ¯
        if 'epoch' in log_dict:
            epoch_info = {
                'epoch': log_dict['epoch'],
                'timestamp': log_dict_copy['timestamp']
            }
            if 'loss' in log_dict:
                epoch_info['train_loss'] = log_dict['loss']
            if 'eval_loss' in log_dict:
                epoch_info['eval_loss'] = log_dict['eval_loss']
            self.epoch_logs.append(epoch_info)
    
    def print_epoch_summary(self, epoch, train_loss, eval_loss=None):
        """æ‰“å°epochæ€»ç»“"""
        print("\n" + "="*60)
        print(f"ğŸ‰ Epoch {epoch} å®Œæˆ!")
        print(f"   è®­ç»ƒæ­¥æ•°: {len(self.train_losses)}")
        print(f"   å¹³å‡è®­ç»ƒLoss: {train_loss:.4f}")
        if eval_loss is not None:
            print(f"   éªŒè¯Loss: {eval_loss:.4f}")
        print("="*60)
    
    def save_all_plots(self):
        """ä¿å­˜æ‰€æœ‰å›¾è¡¨"""
        if not self.train_losses:
            print("âš ï¸ æ²¡æœ‰è®­ç»ƒæ—¥å¿—æ•°æ®")
            return
        
        plt.style.use('seaborn-v0_8-darkgrid')
        fig = plt.figure(figsize=(20, 12))
        
        # 1. è®­ç»ƒæŸå¤±
        ax1 = plt.subplot(2, 2, 1)
        ax1.plot(self.train_steps, self.train_losses, 'b-', linewidth=1, alpha=0.7, label='Training Loss')
        
        # æ·»åŠ å¹³æ»‘æ›²çº¿
        if len(self.train_losses) > 10:
            window = min(50, len(self.train_losses) // 10)
            smooth_loss = pd.Series(self.train_losses).rolling(window=window, min_periods=1).mean()
            ax1.plot(self.train_steps, smooth_loss, 'r-', linewidth=2, alpha=0.9, label=f'Smoothed (window={window})')
        
        ax1.set_title('Training Loss Progression', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Training Steps', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=10)
        
        # 2. éªŒè¯æŸå¤±
        if self.eval_losses:
            ax2 = plt.subplot(2, 2, 2)
            ax2.plot(range(1, len(self.eval_losses)+1), self.eval_losses, 
                    'r-o', linewidth=2, markersize=8, label='Validation Loss')
            ax2.set_title('Validation Loss per Epoch', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Epoch', fontsize=12)
            ax2.set_ylabel('Validation Loss', fontsize=12)
            ax2.grid(True, alpha=0.3)
            ax2.legend(fontsize=10)
            
            # æ ‡è®°æœ€ä½³epoch
            if self.eval_losses:
                best_epoch = np.argmin(self.eval_losses) + 1
                best_loss = min(self.eval_losses)
                ax2.plot(best_epoch, best_loss, 'g*', markersize=15, label=f'Best (Epoch {best_epoch})')
                ax2.legend(fontsize=10)
        
        # 3. å­¦ä¹ ç‡å˜åŒ–
        if self.learning_rates:
            ax3 = plt.subplot(2, 2, 3)
            ax3.plot(range(len(self.learning_rates)), self.learning_rates, 'g-', linewidth=2, alpha=0.8)
            ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
            ax3.set_xlabel('Logging Steps', fontsize=12)
            ax3.set_ylabel('Learning Rate', fontsize=12)
            ax3.grid(True, alpha=0.3)
        
        # 4. è®­ç»ƒè¿‡ç¨‹æ‘˜è¦
        ax4 = plt.subplot(2, 2, 4)
        ax4.axis('off')
        
        # ä¿®å¤æ ¼å¼å­—ç¬¦ä¸²é—®é¢˜ï¼šä½¿ç”¨æ˜¾å¼çš„å­—ç¬¦ä¸²æ ¼å¼åŒ–
        initial_loss = self.train_losses[0] if self.train_losses else 0
        final_loss = self.train_losses[-1] if self.train_losses else 0
        loss_decrease = initial_loss - final_loss if len(self.train_losses) > 1 else 0
        
        summary_text = f"""è®­ç»ƒè¿‡ç¨‹æ‘˜è¦:

æ€»è®­ç»ƒæ­¥æ•°: {len(self.train_losses)}
æ€»éªŒè¯æ¬¡æ•°: {len(self.eval_losses)}

åˆå§‹è®­ç»ƒLoss: {initial_loss:.4f}
æœ€ç»ˆè®­ç»ƒLoss: {final_loss:.4f}
Lossä¸‹é™: {loss_decrease:.4f}

è®­ç»ƒå¼€å§‹: {self.logs_history[0]['timestamp'] if self.logs_history else 'N/A'}
è®­ç»ƒç»“æŸ: {self.logs_history[-1]['timestamp'] if self.logs_history else 'N/A'}
"""
        
        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,
                fontsize=11, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(self.output_dir, 'training_plots.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ è®­ç»ƒåˆ†æå›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        # ä¿å­˜æ—¥å¿—æ•°æ®ä¸ºCSV
        if self.logs_history:
            df = pd.DataFrame(self.logs_history)
            csv_path = os.path.join(self.output_dir, 'training_logs.csv')
            df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"ğŸ“Š æ—¥å¿—æ•°æ®å·²ä¿å­˜: {csv_path}")

class EnhancedLoggingCallback(TrainerCallback):
    """å¢å¼ºçš„æ—¥å¿—è®°å½•å›è°ƒ"""
    def __init__(self, plotter):
        super().__init__()
        self.plotter = plotter
        self.current_step = 0
        self.current_epoch = 0
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            # è®°å½•å½“å‰æ­¥æ•°
            if state and hasattr(state, 'global_step'):
                self.current_step = state.global_step
                self.current_epoch = state.epoch
            
            # æ·»åŠ æ­¥æ•°ä¿¡æ¯
            logs['global_step'] = self.current_step
            logs['epoch'] = self.current_epoch
            
            # ä¼ é€’ç»™plotter
            self.plotter.add_log(logs, step=self.current_step, epoch=self.current_epoch)
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """æ¯ä¸ªepochå¼€å§‹æ—¶è°ƒç”¨"""
        if state:
            self.current_epoch = state.epoch
            print(f"\n{'='*70}")
            print(f"ğŸ“… å¼€å§‹ Epoch {int(self.current_epoch)+1}/{args.num_train_epochs}")
            print(f"{'='*70}")
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """æ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨"""
        if state:
            print(f"\n{'='*70}")
            print(f"ğŸ‰ Epoch {int(state.epoch)+1} å®Œæˆ!")
            print(f"   ç´¯è®¡è®­ç»ƒæ­¥æ•°: {state.global_step}")
            print(f"   å­¦ä¹ ç‡: {state.log_history[-1].get('learning_rate', 'N/A') if state.log_history else 'N/A'}")
            print(f"{'='*70}")

def finetune_qwen():
    """å¾®è°ƒQwen2.5-7B-Instructæ¨¡å‹ - ä¼˜åŒ–åŠ é€Ÿç‰ˆ"""
    
    print("=" * 70)
    print("ğŸš€ Qwen2.5-7B-Instruct åŒ»ç–—é¢†åŸŸå¾®è°ƒï¼ˆä¼˜åŒ–åŠ é€Ÿç‰ˆï¼‰")
    print("=" * 70)
    
    # 1. è®¾ç½®è·¯å¾„
    base_model = "Qwen/Qwen2.5-7B-Instruct"
    dataset_path = "/amax/home/yhji/LM-Course/processed_data"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"/amax/home/yhji/LM-Course/finetuned_model_{timestamp}"
    plots_dir = os.path.join(output_dir, "plots")
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(plots_dir, exist_ok=True)
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {plots_dir}")
    
    # 2. æ£€æŸ¥ç¡¬ä»¶é…ç½®
    print("\nğŸ–¥ï¸ ç¡¬ä»¶é…ç½®æ£€æŸ¥:")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"GPUæ•°é‡: {gpu_count}")
        for i in range(gpu_count):
            gpu_props = torch.cuda.get_device_properties(i)
            print(f"  GPU {i}: {gpu_props.name}")
            print(f"    æ˜¾å­˜: {gpu_props.total_memory / 1024**3:.1f} GB")
        print(f"æ”¯æŒbfloat16: {torch.cuda.is_bf16_supported()}")
    else:
        print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
    
    # 3. åˆå§‹åŒ–å›¾è¡¨è®°å½•å™¨
    plotter = TrainingPlotter(plots_dir)
    
    # 4. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(dataset_path)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒé›†: {len(dataset['train'])} æ¡")
    print(f"  éªŒè¯é›†: {len(dataset['test'])} æ¡")
    
    # 5. åŠ è½½tokenizer
    print("\nğŸ”¤ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model,
        trust_remote_code=True,
        padding_side="right"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 6. æ•°æ®é¢„å¤„ç†å‡½æ•°
    def tokenize_function(example):
        """Tokenizeå¯¹è¯æ•°æ®"""
        conversations = example["conversations"]
        
        # ä½¿ç”¨Qwençš„èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            conversations,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # Tokenize - å‡å°‘æœ€å¤§é•¿åº¦ä»¥åŠ é€Ÿ
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=512,  # å‡å°‘åˆ°512ä»¥åŠ é€Ÿ
            padding=False
        )
        
        # è®¾ç½®labelsï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized
    
    print("ğŸ”§ Tokenizingæ•°æ®é›†...")
    tokenized_dataset = dataset.map(
        tokenize_function,
        remove_columns=dataset["train"].column_names,
        desc="Tokenizing",
        num_proc=4  # å¤šè¿›ç¨‹å¤„ç†
    )
    
    # ä¼˜åŒ–æ•°æ®é›†æ ¼å¼
    tokenized_dataset = tokenized_dataset.with_format(
        "torch",
        columns=["input_ids", "attention_mask", "labels"]
    )
    
    # 7. åŠ è½½æ¨¡å‹ï¼ˆå…³é”®ä¼˜åŒ–ï¼šå¯ç”¨Flash Attention 2ï¼‰
    print("\nğŸ¤– åŠ è½½æ¨¡å‹...")
    
    # æ ¹æ®GPUæƒ…å†µé€‰æ‹©ç²¾åº¦
    if torch.cuda.is_available():
        print(f"ğŸ® GPUå¯ç”¨: {torch.cuda.device_count()}ä¸ª")
        
        # ä¼˜å…ˆä½¿ç”¨bfloat16
        if torch.cuda.is_bf16_supported():
            torch_dtype = torch.bfloat16
            print("ğŸ“ ä½¿ç”¨ç²¾åº¦: bfloat16")
        else:
            torch_dtype = torch.float16
            print("ğŸ“ ä½¿ç”¨ç²¾åº¦: float16")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attention 2
        try:
            model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch_dtype,
                device_map="auto",
                trust_remote_code=True,
                use_cache=False,
                attn_implementation="flash_attention_2",  # å…³é”®åŠ é€Ÿï¼
            )
            print("âœ… å·²å¯ç”¨Flash Attention 2 (å¤§å¹…åŠ é€Ÿ)")
        except:
            print("âš ï¸ Flash Attention 2ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›")
            model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch_dtype,
                device_map="auto",
                trust_remote_code=True,
                use_cache=False,
            )
    else:
        torch_dtype = torch.float32
        print("âš ï¸ ä½¿ç”¨CPUæ¨¡å¼")
        model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch_dtype,
            device_map=None,
            trust_remote_code=True,
            use_cache=False,
        )
    
    # å¯ç”¨è¾“å…¥æ¢¯åº¦
    model.enable_input_require_grads()
    
    # ç¡®ä¿æ‰€æœ‰å‚æ•°éœ€è¦æ¢¯åº¦
    for param in model.parameters():
        param.requires_grad = True
    
    print("âœ… å·²å¯ç”¨æ¨¡å‹æ¢¯åº¦")
    
    # 8. é…ç½®LoRA
    print("\nğŸ¯ é…ç½®LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=32,
        lora_alpha=64,
        lora_dropout=0.1,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        bias="none",
        inference_mode=False
    )
    
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    print(f"ğŸ“Š æ€»å‚æ•°: {total_params:,}")
    
    # 9. è®­ç»ƒå‚æ•° - ä¼˜åŒ–ç‰ˆ
    print("\nâš™ï¸ è®¾ç½®è®­ç»ƒå‚æ•°ï¼ˆä¼˜åŒ–åŠ é€Ÿæ¨¡å¼ï¼‰...")
    
    # è®¡ç®—æ€»æ­¥æ•°
    train_dataset_size = len(tokenized_dataset["train"])
    per_device_batch = 4  # æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼š2, 4, 8
    gradient_accumulation = 2  # å‡å°‘æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    effective_batch = per_device_batch * gradient_accumulation
    
    # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå‡å°‘batch size
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:  # å°äº20GBæ˜¾å­˜
            per_device_batch = 2
            gradient_accumulation = 4
            effective_batch = per_device_batch * gradient_accumulation
            print(f"âš ï¸ æ˜¾å­˜è¾ƒå°({gpu_memory:.1f}GB)ï¼Œä½¿ç”¨batch_size={per_device_batch}")
    
    total_steps = (train_dataset_size * 3) // effective_batch  # 3ä¸ªepoch
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=per_device_batch,
        per_device_eval_batch_size=8,  # è¯„ä¼°ç”¨æ›´å¤§batch
        gradient_accumulation_steps=gradient_accumulation,
        warmup_steps=100,
        learning_rate=1e-4,
        fp16=torch_dtype == torch.float16,
        bf16=torch_dtype == torch.bfloat16,
        logging_steps=20,               # å‡å°‘æ—¥å¿—é¢‘ç‡
        eval_strategy="steps",          # æŒ‰æ­¥è¯„ä¼°è€Œä¸æ˜¯æŒ‰epoch
        eval_steps=500,                 # æ¯500æ­¥è¯„ä¼°ä¸€æ¬¡
        save_strategy="steps",
        save_steps=500,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_total_limit=3,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},  # æ–°ç‰ˆæœ¬ç”¨æ³•
        optim="adamw_torch_fused",      # ä½¿ç”¨èåˆä¼˜åŒ–å™¨åŠ é€Ÿ
        logging_dir=os.path.join(output_dir, "logs"),
        group_by_length=True,
        lr_scheduler_type="cosine",
        report_to=[],
        ddp_find_unused_parameters=False,
        logging_first_step=True,
        logging_nan_inf_filter=False,
        
        # æ•°æ®åŠ è½½ä¼˜åŒ–
        dataloader_num_workers=4,       # å¢åŠ æ•°æ®åŠ è½½è¿›ç¨‹
        dataloader_prefetch_factor=2,   # é¢„å–æ•°æ®
        remove_unused_columns=True,     # ç§»é™¤æœªç”¨åˆ—
        
        # é˜²è¿‡æ‹Ÿåˆå‚æ•°
        weight_decay=0.01,
        max_grad_norm=1.0,
        label_smoothing_factor=0.0,
        
        # å†…å­˜ä¼˜åŒ–
        eval_accumulation_steps=1,
        # fsdp="auto_wrap" if torch.cuda.device_count() > 1 else None,  # å¤šGPUæ—¶å¯ç”¨
    )
    
    # 10. æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        padding=True,
        pad_to_multiple_of=8,  # å¯¹é½åˆ°8çš„å€æ•°ï¼ŒåŠ é€Ÿè®¡ç®—
    )
    
    # 11. æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print("\n" + "="*70)
    print("ğŸ“‹ è®­ç»ƒé…ç½®æ‘˜è¦:")
    print("="*70)
    print(f"1. è®­ç»ƒæ¨¡å¼: ä¼˜åŒ–åŠ é€Ÿæ¨¡å¼")
    print(f"   â€¢ è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
    print(f"   â€¢ Batchå¤§å°: {per_device_batch} Ã— {gradient_accumulation} = {effective_batch}")
    print(f"   â€¢ æ€»è®­ç»ƒæ­¥æ•°: ~{total_steps}")
    print(f"   â€¢ åºåˆ—é•¿åº¦: 512")
    
    print(f"\n2. æ¨¡å‹é…ç½®:")
    print(f"   â€¢ LoRAç§©: {lora_config.r}")
    print(f"   â€¢ å¯è®­ç»ƒå‚æ•°: {trainable_params/total_params*100:.2f}%")
    
    print(f"\n3. ä¼˜åŒ–é…ç½®:")
    print(f"   â€¢ Flash Attention 2: å·²å¯ç”¨")
    print(f"   â€¢ ä¼˜åŒ–å™¨: {training_args.optim}")
    print(f"   â€¢ æ•°æ®åŠ è½½è¿›ç¨‹: {training_args.dataloader_num_workers}")
    
    print(f"\n4. ç­–ç•¥é…ç½®:")
    print(f"   â€¢ è¯„ä¼°é¢‘ç‡: æ¯{training_args.eval_steps}æ­¥")
    print(f"   â€¢ ä¿å­˜é¢‘ç‡: æ¯{training_args.save_steps}æ­¥")
    print(f"   â€¢ æ—¥å¿—é¢‘ç‡: æ¯{training_args.logging_steps}æ­¥")
    print("="*70)
    
    # 12. åˆ›å»ºTrainer
    print("\nğŸ¤– åˆ›å»ºTrainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        data_collator=data_collator,
        callbacks=[
            EnhancedLoggingCallback(plotter),
            EarlyStoppingCallback(
                early_stopping_patience=3,
                early_stopping_threshold=0.001
            )
        ],
    )
    
    # å•ç‹¬è®¾ç½®tokenizer
    trainer.tokenizer = tokenizer
    
    # 13. æ€§èƒ½æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    if torch.cuda.is_available():
        print("\nâš¡ æ€§èƒ½æµ‹è¯•...")
        try:
            # æµ‹è¯•ä¸€ä¸ªbatchçš„å‰å‘ä¼ æ’­
            test_batch = next(iter(trainer.get_train_dataloader()))
            for k, v in test_batch.items():
                if isinstance(v, torch.Tensor):
                    test_batch[k] = v.to(model.device)
            
            import time
            start = time.time()
            with torch.no_grad():
                outputs = model(**test_batch)
            end = time.time()
            print(f"å•batchå‰å‘æ—¶é—´: {(end-start)*1000:.1f}ms")
        except:
            pass
    
    # 14. å¼€å§‹è®­ç»ƒï¼
    print("\n" + "="*70)
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    try:
        train_result = trainer.train()
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        plotter.save_all_plots()
        return output_dir
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return output_dir
    
    # 15. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    with open(os.path.join(output_dir, "training_results.json"), "w", encoding='utf-8') as f:
        json.dump(train_result.metrics, f, indent=2, ensure_ascii=False)
    
    # 16. ä¿å­˜å›¾è¡¨
    print("\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒå›¾è¡¨...")
    plotter.save_all_plots()
    
    # 17. æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°...")
    eval_results = trainer.evaluate()
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_file = os.path.join(output_dir, "final_evaluation.json")
    with open(eval_file, "w", encoding='utf-8') as f:
        json.dump(eval_results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {output_dir}")
    print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {eval_results.get('eval_loss', 'N/A'):.4f}")
    print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {train_result.global_step}")
    print(f"ğŸ“Š æ€»è®­ç»ƒè½®æ•°: {train_result.epoch}")
    print(f"ğŸ“Š è®­ç»ƒæ—¶é—´: {train_result.metrics.get('train_runtime', 0):.1f}ç§’")
    
    if torch.cuda.is_available():
        print(f"\nğŸ® GPUä½¿ç”¨ç»Ÿè®¡:")
        for i in range(torch.cuda.device_count()):
            mem_used = torch.cuda.memory_allocated(i) / 1024**3
            mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {mem_used:.1f}/{mem_total:.1f} GB ({mem_used/mem_total*100:.1f}%)")
    
    # è®¡ç®—å¹³å‡é€Ÿåº¦
    if train_result.metrics.get('train_runtime', 0) > 0:
        steps_per_second = train_result.global_step / train_result.metrics['train_runtime']
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {steps_per_second:.2f} æ­¥/ç§’, {1/steps_per_second:.2f} ç§’/æ­¥")
    
    return output_dir

if __name__ == "__main__":
    finetune_qwen()
    