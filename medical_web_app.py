# medical_web_app.py - ç®€åŒ–ä¾§è¾¹æ ç‰ˆæœ¬
import streamlit as st
import torch
import os
import sys
import json
import re
import time
from datetime import datetime
from pathlib import Path
import glob

# ========== ç¯å¢ƒè®¾ç½® ==========
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = "/amax/home/yhji/LM-Course"

# ========== å¯¼å…¥ç›¸å…³åº“ ==========
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    from peft import PeftModel, PeftConfig
    print("âœ… æ¨¡å‹åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    st.error(f"âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    st.stop()

# ========== Streamlité¡µé¢é…ç½® ==========
st.set_page_config(
    page_title="ğŸ¥ åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ - Qwen2.5å¾®è°ƒç‰ˆ",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': """
        ### åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ v1.0
        
        **æ¨¡å‹ä¿¡æ¯:**
        - åŸºç¡€æ¨¡å‹: Qwen2.5-7B-Instruct
        - å¾®è°ƒæ–¹æ³•: LoRA (åŒ»ç–—é¢†åŸŸ)
        - è®­ç»ƒæ•°æ®: åŒ»ç–—é—®ç­”å¯¹
        
        **åŠŸèƒ½:**
        - ä¸“ä¸šåŒ»ç–—å’¨è¯¢
        - ç—‡çŠ¶åˆ†æå»ºè®®
        - å¥åº·æŒ‡å¯¼
        
        **å…è´£å£°æ˜:** æœ¬åŠ©æ‰‹æä¾›çš„ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚
        """
    }
)

# ========== è‡ªå®šä¹‰CSSæ ·å¼ ==========
st.markdown("""
<style>
    /* ä¸»æ ‡é¢˜ */
    .main-title {
        text-align: center;
        color: #1a237e;
        padding: 20px;
        background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        border-radius: 10px;
        margin-bottom: 30px;
        border-left: 5px solid #1e88e5;
    }
    
    /* èŠå¤©æ¶ˆæ¯æ ·å¼ */
    .user-message {
        background-color: #e3f2fd;
        padding: 15px 20px;
        border-radius: 18px 18px 4px 18px;
        margin: 10px 0 10px auto;
        max-width: 85%;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        border: 1px solid #bbdefb;
        line-height: 1.6;
    }
    
    .assistant-message {
        background-color: #f8f9fa;
        padding: 20px 25px;
        border-radius: 18px 18px 18px 4px;
        margin: 10px auto 10px 0;
        max-width: 85%;
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        border: 1px solid #e0e0e0;
        border-left: 4px solid #4caf50;
        line-height: 1.8;
    }
    
    /* åŒ»ç–—æŠ¥å‘Šç‰¹æ®Šæ ·å¼ */
    .medical-report {
        font-family: 'SimSun', 'NSimSun', serif;
        color: #333;
    }
    
    .section-title {
        color: #1a237e;
        font-weight: 700;
        font-size: 1.2em;
        margin: 20px 0 10px 0;
        padding-bottom: 5px;
        border-bottom: 2px solid #1e88e5;
    }
    
    .subsection-title {
        color: #0d47a1;
        font-weight: 600;
        font-size: 1.1em;
        margin: 15px 0 8px 0;
    }
    
    .content-text {
        color: #424242;
        margin-left: 15px;
        margin-bottom: 12px;
    }
    
    /* çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-online {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        background-color: #4caf50;
        margin-right: 8px;
        animation: pulse 2s infinite;
    }
    
    @keyframes pulse {
        0% { opacity: 1; }
        50% { opacity: 0.5; }
        100% { opacity: 1; }
    }
    
    .status-offline {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        background-color: #f44336;
        margin-right: 8px;
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {
        border-radius: 8px;
        font-weight: 500;
        transition: all 0.3s;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    
    /* è¾“å…¥æ¡† */
    .stTextArea > div > div > textarea {
        border-radius: 10px;
        padding: 15px;
        font-size: 16px;
        line-height: 1.5;
    }
    
    /* èŠå¤©å®¹å™¨ */
    .chat-container {
        height: 68vh;
        overflow-y: auto;
        padding: 20px;
        background-color: #fafafa;
        border-radius: 10px;
        border: 1px solid #e0e0e0;
        margin-bottom: 20px;
        scroll-behavior: smooth;
    }
    
    /* æ»šåŠ¨æ¡æ ·å¼ */
    .chat-container::-webkit-scrollbar {
        width: 8px;
    }
    
    .chat-container::-webkit-scrollbar-track {
        background: #f1f1f1;
        border-radius: 4px;
    }
    
    .chat-container::-webkit-scrollbar-thumb {
        background: #bdbdbd;
        border-radius: 4px;
    }
    
    .chat-container::-webkit-scrollbar-thumb:hover {
        background: #9e9e9e;
    }
    
    /* å…è´£å£°æ˜ */
    .disclaimer-box {
        background-color: #fff3cd;
        padding: 15px 20px;
        border-radius: 10px;
        border-left: 4px solid #ffc107;
        margin-top: 20px;
        font-size: 0.9em;
    }
    
    /* å°æ ‡é¢˜ */
    .sub-heading {
        color: #546e7a;
        font-size: 0.95em;
        margin-top: 5px;
    }
    
    /* ç®€åŒ–ç‰ˆä¾§è¾¹æ æ ·å¼ */
    .simple-sidebar {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 10px;
        margin-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

# ========== æ¨¡å‹åŠ è½½ç±» ==========
class MedicalModel:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = None
        self.model_loaded = False
        self.model_info = {}
        
    def load_model(self, model_path=None):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
        try:
            st.sidebar.info("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
            
            # æ¸…ç©ºGPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                st.sidebar.info("ğŸ”„ å·²æ¸…ç©ºGPUç¼“å­˜")
            
            # 1. è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
            if model_path is None:
                model_dirs = glob.glob(f"{PROJECT_ROOT}/finetuned_model_*")
                if not model_dirs:
                    st.sidebar.error("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
                    return False
                
                model_dirs.sort(key=os.path.getmtime, reverse=True)
                model_path = model_dirs[0]
                self.model_info['path'] = os.path.basename(model_path)
                st.sidebar.success(f"ğŸ“‚ ä½¿ç”¨æ¨¡å‹: {self.model_info['path']}")
            
            # 2. é…ç½®4ä½é‡åŒ–ä»¥å‡å°‘æ˜¾å­˜
            st.sidebar.info("âš™ï¸ é…ç½®é‡åŒ–åŠ è½½...")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )
            
            # 3. åŠ è½½tokenizer
            st.sidebar.info("ğŸ”¤ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                "Qwen/Qwen2.5-7B-Instruct",
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # 4. ä½¿ç”¨4ä½é‡åŒ–åŠ è½½åŸºç¡€æ¨¡å‹
            st.sidebar.info("ğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹ (4ä½é‡åŒ–)...")
            self.model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-7B-Instruct",
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                use_cache=False
            )
            
            # 5. åŠ è½½LoRAé€‚é…å™¨
            st.sidebar.info("ğŸ¯ åŠ è½½LoRAé€‚é…å™¨...")
            self.model = PeftModel.from_pretrained(self.model, model_path)
            self.model.eval()
            
            # 6. è®¾ç½®è®¾å¤‡
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                st.sidebar.success(f"âœ… æ¨¡å‹åŠ è½½åˆ°: {self.device}")
                
                # æ˜¾å­˜ç»Ÿè®¡
                allocated = torch.cuda.memory_allocated() / 1024**3
                self.model_info['gpu_memory'] = allocated
                st.sidebar.info(f"ğŸ“Š æ˜¾å­˜å ç”¨: {allocated:.2f} GB")
            else:
                self.device = torch.device("cpu")
                st.sidebar.warning("âš ï¸ ä½¿ç”¨CPUæ¨¡å¼")
            
            self.model_loaded = True
            
            # æµ‹è¯•æ¨ç†
            st.sidebar.info("ğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†...")
            test_text = "æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"
            test_input = self.tokenizer(test_text, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                _ = self.model.generate(**test_input, max_new_tokens=20)
            
            st.sidebar.success("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
            
            return True
                
        except Exception as e:
            st.sidebar.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)[:200]}...")
            import traceback
            traceback.print_exc()
            return False
    
    def format_response(self, text):
        """æ ¼å¼åŒ–æ¨¡å‹è¾“å‡ºï¼Œç¡®ä¿æ­£ç¡®çš„æ ‡é¢˜æ ¼å¼"""
        # æ¸…ç†å¤šä½™çš„Markdownç¬¦å·
        text = re.sub(r'#{2,}', '', text)  # ç§»é™¤å¤šä¸ª#
        text = re.sub(r'\*\*\s*', '', text)  # ç§»é™¤**åŠ ç©ºæ ¼
        text = re.sub(r'\*\*([^\\*]+)\*\*', r'ã€\1ã€‘', text)  # è½¬æ¢**å†…å®¹**ä¸ºã€å†…å®¹ã€‘
        
        # ç»Ÿä¸€æ ‡é¢˜æ ¼å¼
        patterns = [
            (r'^[\s]*ä¸€[ã€.]?\s*ç—…æƒ…åˆ†æ', 'ã€ä¸€ã€ç—…æƒ…åˆ†æã€‘'),
            (r'^[\s]*äºŒ[ã€.]?\s*åŸå› åˆ†æ', 'ã€äºŒã€åŸå› åˆ†æã€‘'),
            (r'^[\s]*ä¸‰[ã€.]?\s*æ²»ç—…å»ºè®®', 'ã€ä¸‰ã€æ²»ç—…å»ºè®®ã€‘'),
            (r'^[\s]*1[ã€.]?\s*ç—‡çŠ¶å…¨é¢è¯„ä¼°', 'ã€1. ç—‡çŠ¶å…¨é¢è¯„ä¼°ï¼šã€‘'),
            (r'^[\s]*2[ã€.]?\s*å¯èƒ½ç–¾ç—…åˆ¤æ–­', 'ã€2. å¯èƒ½ç–¾ç—…åˆ¤æ–­ï¼šã€‘'),
            (r'^[\s]*1[ã€.]?\s*ä¸»è¦ç—…å› è§£æ', 'ã€1. ä¸»è¦ç—…å› è§£æï¼šã€‘'),
            (r'^[\s]*2[ã€.]?\s*é‰´åˆ«è¯Šæ–­è¦ç‚¹', 'ã€2. é‰´åˆ«è¯Šæ–­è¦ç‚¹ï¼šã€‘'),
            (r'^[\s]*1[ã€.]?\s*å°±åŒ»æŒ‡å¯¼', 'ã€1. å°±åŒ»æŒ‡å¯¼ï¼šã€‘'),
            (r'^[\s]*2[ã€.]?\s*æ²»ç–—æ–¹æ¡ˆå»ºè®®', 'ã€2. æ²»ç–—æ–¹æ¡ˆå»ºè®®ï¼šã€‘'),
        ]
        
        lines = text.split('\n')
        formatted_lines = []
        
        for line in lines:
            formatted_line = line
            for pattern, replacement in patterns:
                formatted_line = re.sub(pattern, replacement, formatted_line, flags=re.IGNORECASE)
            formatted_lines.append(formatted_line)
        
        # æ·»åŠ æ¢è¡Œå’Œç¼©è¿›
        formatted_text = '\n'.join(formatted_lines)
        formatted_text = formatted_text.replace('ã€ä¸€ã€ç—…æƒ…åˆ†æã€‘', '\nã€ä¸€ã€ç—…æƒ…åˆ†æã€‘')
        formatted_text = formatted_text.replace('ã€äºŒã€åŸå› åˆ†æã€‘', '\n\nã€äºŒã€åŸå› åˆ†æã€‘')
        formatted_text = formatted_text.replace('ã€ä¸‰ã€æ²»ç—…å»ºè®®ã€‘', '\n\nã€ä¸‰ã€æ²»ç—…å»ºè®®ã€‘')
        
        return formatted_text.strip()
    
    def generate_response(self, user_input, max_tokens=800):
        """ç”Ÿæˆå›ç­”"""
        if not self.model_loaded:
            return "âš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
        
        try:
            # ğŸ› ï¸ ä¸¥æ ¼çš„æ ¼å¼æŒ‡ä»¤
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„ç—‡çŠ¶æè¿°ç»™å‡ºä¸“ä¸šã€æ¸…æ™°ä¸”å®ç”¨çš„å¥åº·å»ºè®®ã€‚

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼ˆä¸è¦ä½¿ç”¨ä»»ä½•Markdownç¬¦å·å¦‚#ã€*ã€**ç­‰ï¼‰ï¼š**

ã€ä¸€ã€ç—…æƒ…åˆ†æã€‘
ã€1. ç—‡çŠ¶å…¨é¢è¯„ä¼°ï¼šã€‘
ã€2. å¯èƒ½ç–¾ç—…åˆ¤æ–­ï¼šã€‘

ã€äºŒã€åŸå› åˆ†æã€‘
ã€1. ä¸»è¦ç—…å› è§£æï¼šã€‘
ã€2. é‰´åˆ«è¯Šæ–­è¦ç‚¹ï¼šã€‘

ã€ä¸‰ã€æ²»ç—…å»ºè®®ã€‘
ã€1. å°±åŒ»æŒ‡å¯¼ï¼šã€‘
ã€2. æ²»ç–—æ–¹æ¡ˆå»ºè®®ï¼šã€‘

è§„åˆ™ï¼š
1. ä½¿ç”¨ã€ã€‘åŒ…è£¹æ‰€æœ‰æ ‡é¢˜
2. æ¯ä¸ªæ ‡é¢˜å•ç‹¬ä¸€è¡Œ
3. æ ‡é¢˜åæ¢è¡Œå†å†™å…·ä½“å†…å®¹
4. å†…å®¹è¦è¯¦ç»†å…·ä½“ï¼Œæ¯ä¸ªéƒ¨åˆ†è‡³å°‘2-3å¥è¯
5. ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ¼å¼ç¬¦å·
"""
            
            # å¼ºåŒ–æ ¼å¼è¦æ±‚
            formatted_user_input = f"ç”¨æˆ·ç—‡çŠ¶æè¿°ï¼š{user_input}\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¦æ±‚å›ç­”ï¼Œä½¿ç”¨ã€ã€‘åŒ…è£¹æ ‡é¢˜ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•Markdownç¬¦å·ã€‚"
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": formatted_user_input}
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            try:
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except:
                # å¤‡ç”¨æ¨¡æ¿
                text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{formatted_user_input}<|im_end|>\n<|im_start|>assistant\n"
            
            # Tokenize
            inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
            input_length = inputs['input_ids'].shape[1]
            
            # ç”Ÿæˆï¼ˆå›ºå®šå‚æ•°ï¼‰
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.5,  # å›ºå®šæ¸©åº¦
                    top_p=0.85,
                    do_sample=True,
                    repetition_penalty=1.2,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id,
                    num_beams=1,
                    no_repeat_ngram_size=3
                )
            
            # è§£ç 
            generated_ids = outputs[0][input_length:]  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–
            response = self.format_response(response)
            
            # å¦‚æœæ ¼å¼ä»ç„¶æœ‰é—®é¢˜ï¼Œè¿›è¡Œæœ€ç»ˆæ¸…ç†
            response = response.replace('###', '').replace('####', '').replace('#####', '')
            response = re.sub(r'\n\s*\n\s*\n+', '\n\n', response)  # ç§»é™¤å¤šä½™ç©ºè¡Œ
            
            return response
            
        except Exception as e:
            return f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)[:100]}"

# ========== åˆå§‹åŒ–æ¨¡å‹ ==========
@st.cache_resource(show_spinner=False)
def init_model():
    """åˆå§‹åŒ–æ¨¡å‹ï¼ˆç¼“å­˜ï¼‰"""
    with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½åŒ»ç–—AIæ¨¡å‹..."):
        model = MedicalModel()
        success = model.load_model()
        return model if success else None

# ========== ç®€åŒ–ä¾§è¾¹æ é…ç½® ==========
with st.sidebar:
    st.markdown("### âš™ï¸ æ§åˆ¶é¢æ¿")
    
    # æ¨¡å‹çŠ¶æ€
    st.markdown("#### ğŸ“Š æ¨¡å‹çŠ¶æ€")
    col1, col2 = st.columns([1, 3])
    with col1:
        if torch.cuda.is_available():
            st.markdown('<div class="status-online"></div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="status-offline"></div>', unsafe_allow_html=True)
    with col2:
        status_text = "**åœ¨çº¿ (GPU)**" if torch.cuda.is_available() else "**ç¦»çº¿ (CPU)**"
        st.markdown(status_text)
    
    # ä¼šè¯ç®¡ç†
    st.markdown("#### ğŸ’¾ ä¼šè¯ç®¡ç†")
    if st.button("ğŸ”„ æ¸…ç©ºå¯¹è¯", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
    
    # å¯¼å‡ºå¯¹è¯
    if st.button("ğŸ’¾ å¯¼å‡ºä¸ºJSON", use_container_width=True):
        if 'messages' in st.session_state and st.session_state.messages:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_export_{timestamp}.json"
            export_dir = os.path.join(PROJECT_ROOT, "chat_exports")
            os.makedirs(export_dir, exist_ok=True)
            export_path = os.path.join(export_dir, filename)
            
            export_data = {
                "export_time": datetime.now().isoformat(),
                "total_messages": len(st.session_state.messages),
                "model": "Qwen2.5-7B-Instruct-Finetuned-Medical",
                "messages": st.session_state.messages
            }
            
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            st.sidebar.success(f"âœ… å·²å¯¼å‡ºåˆ°:\n`{export_path}`")

# ========== ä¸»é¡µé¢ ==========
# æ ‡é¢˜åŒºåŸŸ
st.markdown("""
<div class="main-title">
    <h1>ğŸ¥ åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹</h1>
    <p class="sub-heading">åŸºäº Qwen2.5-7B-Instruct å¾®è°ƒçš„åŒ»ç–—é—®ç­”ç³»ç»Ÿ</p>
</div>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'messages' not in st.session_state:
    st.session_state.messages = []
    
    # æ·»åŠ æ¬¢è¿æ¶ˆæ¯
    welcome_msg = """æ‚¨å¥½ï¼æˆ‘æ˜¯ä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›å¥åº·å’¨è¯¢å’Œå»ºè®®ã€‚

**ä½¿ç”¨è¯´æ˜ï¼š**
1. è¯¦ç»†æè¿°æ‚¨çš„ç—‡çŠ¶æˆ–å¥åº·é—®é¢˜
2. æˆ‘ä¼šæŒ‰ç…§æ ‡å‡†åŒ»ç–—æŠ¥å‘Šæ ¼å¼ä¸ºæ‚¨åˆ†æ
3. æˆ‘çš„å›ç­”å°†åŒ…å«ç—…æƒ…åˆ†æã€åŸå› åˆ†æå’Œæ²»ç–—å»ºè®®

**è¯·æ³¨æ„ï¼š** æˆ‘çš„å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­ã€‚"""
    
    st.session_state.messages.append({
        "role": "assistant", 
        "content": welcome_msg,
        "timestamp": datetime.now().isoformat()
    })

# åˆå§‹åŒ–æ¨¡å‹
model = init_model()

if model is None:
    st.error("""
    âš ï¸ **æ¨¡å‹åŠ è½½å¤±è´¥ï¼**
    
    å¯èƒ½çš„åŸå› ï¼š
    1. æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
    2. GPUæ˜¾å­˜ä¸è¶³
    3. ä¾èµ–åº“æœªæ­£ç¡®å®‰è£…
    
    è¯·æ£€æŸ¥ï¼š
    - æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
    - ç¡®ä¿å·²å®Œæˆæ¨¡å‹è®­ç»ƒ
    - é‡å¯åº”ç”¨æˆ–æœåŠ¡å™¨
    """)
    st.stop()

# èŠå¤©æ˜¾ç¤ºåŒºåŸŸ
chat_container = st.container()
with chat_container:
    st.markdown('<div class="chat-container" id="chat-container">', unsafe_allow_html=True)
    
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f"""
            <div class="user-message">
                <strong>ğŸ‘¤ æ‚¨:</strong><br>
                {message['content']}
            </div>
            """, unsafe_allow_html=True)
        elif message["role"] == "assistant":
            content = message['content']
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåŒ»ç–—æŠ¥å‘Šæ ¼å¼
            if "ã€ä¸€ã€ç—…æƒ…åˆ†æã€‘" in content:
                # ä½¿ç”¨åŒ»ç–—æŠ¥å‘Šæ ·å¼
                st.markdown(f"""
                <div class="assistant-message medical-report">
                    <strong>ğŸ¥ åŒ»ç–—æŠ¥å‘Š:</strong><br><br>
                """, unsafe_allow_html=True)
                
                # åˆ†å‰²å†…å®¹
                sections = re.split(r'ã€[ä¸€äºŒä¸‰]ã€[^ã€‘]+ã€‘', content)
                titles = re.findall(r'ã€[ä¸€äºŒä¸‰]ã€[^ã€‘]+ã€‘', content)
                
                # æ˜¾ç¤ºæ¯ä¸ªéƒ¨åˆ†
                for i, (title, section) in enumerate(zip(titles, sections[1:] if len(sections) > 1 else [content])):
                    # å¤„ç†ç—…æƒ…åˆ†æ
                    if i == 0 and "ä¸€ã€ç—…æƒ…åˆ†æ" in title:
                        st.markdown(f'<div class="section-title">{title}</div>', unsafe_allow_html=True)
                        # æå–å­éƒ¨åˆ†
                        subsections = re.split(r'ã€[0-9]+\.[^ã€‘]+ã€‘', section)
                        subtitles = re.findall(r'ã€[0-9]+\.[^ã€‘]+ã€‘', section)
                        
                        for j, (subtitle, subcontent) in enumerate(zip(subtitles, subsections[1:] if len(subsections) > 1 else [section])):
                            st.markdown(f'<div class="subsection-title">{subtitle}</div>', unsafe_allow_html=True)
                            st.markdown(f'<div class="content-text">{subcontent.strip()}</div>', unsafe_allow_html=True)
                    
                    # å¤„ç†åŸå› åˆ†æ
                    elif i == 1 and "äºŒã€åŸå› åˆ†æ" in title:
                        st.markdown(f'<div class="section-title">{title}</div>', unsafe_allow_html=True)
                        subsections = re.split(r'ã€[0-9]+\.[^ã€‘]+ã€‘', section)
                        subtitles = re.findall(r'ã€[0-9]+\.[^ã€‘]+ã€‘', section)
                        
                        for j, (subtitle, subcontent) in enumerate(zip(subtitles, subsections[1:] if len(subsections) > 1 else [section])):
                            st.markdown(f'<div class="subsection-title">{subtitle}</div>', unsafe_allow_html=True)
                            st.markdown(f'<div class="content-text">{subcontent.strip()}</div>', unsafe_allow_html=True)
                    
                    # å¤„ç†æ²»ç–—å»ºè®®
                    elif i == 2 and "ä¸‰ã€æ²»ç—…å»ºè®®" in title:
                        st.markdown(f'<div class="section-title">{title}</div>', unsafe_allow_html=True)
                        subsections = re.split(r'ã€[0-9]+\.[^ã€‘]+ã€‘', section)
                        subtitles = re.findall(r'ã€[0-9]+\.[^ã€‘]+ã€‘', section)
                        
                        for j, (subtitle, subcontent) in enumerate(zip(subtitles, subsections[1:] if len(subsections) > 1 else [section])):
                            st.markdown(f'<div class="subsection-title">{subtitle}</div>', unsafe_allow_html=True)
                            st.markdown(f'<div class="content-text">{subcontent.strip()}</div>', unsafe_allow_html=True)
                
                st.markdown("</div>", unsafe_allow_html=True)
            else:
                # æ™®é€šæ¶ˆæ¯
                st.markdown(f"""
                <div class="assistant-message">
                    <strong>ğŸ¥ åŠ©æ‰‹:</strong><br>
                    {content}
                </div>
                """, unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # JavaScriptè‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
    st.markdown("""
    <script>
        function scrollToBottom() {
            var container = document.querySelector('.chat-container');
            if (container) {
                container.scrollTop = container.scrollHeight;
            }
        }
        // é¡µé¢åŠ è½½æ—¶æ»šåŠ¨
        window.onload = scrollToBottom;
        // Streamlitæ¯æ¬¡æ›´æ–°åæ»šåŠ¨
        setTimeout(scrollToBottom, 100);
    </script>
    """, unsafe_allow_html=True)

# è¾“å…¥åŒºåŸŸ
st.markdown("### ğŸ’¬ è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜")

with st.form(key="chat_form", clear_on_submit=True):
    user_input = st.text_area(
        "",
        placeholder="è¯·è¯¦ç»†æè¿°æ‚¨çš„ç—‡çŠ¶ã€æŒç»­æ—¶é—´ã€ä¼´éšç—‡çŠ¶ç­‰ï¼ˆä¾‹å¦‚ï¼šæˆ‘å¤´ç—›ã€å‘çƒ§3å¤©äº†ï¼Œè¿˜ä¼´æœ‰å’³å—½ï¼Œä½“æ¸©æœ€é«˜38.5â„ƒï¼‰",
        height=120,
        key="user_input",
        max_chars=1000,
        help="æè¿°è¶Šè¯¦ç»†ï¼Œåˆ†æè¶Šå‡†ç¡®"
    )
    
    col1, col2 = st.columns([1, 6])
    with col1:
        submit_button = st.form_submit_button(
            "ğŸš€ å‘é€",
            use_container_width=True,
            help="å‘é€é—®é¢˜ç»™åŒ»ç–—åŠ©æ‰‹"
        )
    with col2:
        st.caption("ğŸ“ å»ºè®®è¯¦ç»†æè¿°ç—‡çŠ¶ï¼ŒæŒ‰ Enter æ¢è¡Œï¼ŒCtrl+Enter å‘é€")

# å¤„ç†ç”¨æˆ·è¾“å…¥
if submit_button and user_input.strip():
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({
        "role": "user",
        "content": user_input.strip(),
        "timestamp": datetime.now().isoformat()
    })
    
    # ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.spinner("ğŸ¤– åŒ»ç–—åŠ©æ‰‹æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
        start_time = time.time()
        response = model.generate_response(
            user_input.strip(),
            max_tokens=800  # å›ºå®šå‚æ•°
        )
        generation_time = time.time() - start_time
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
    st.session_state.messages.append({
        "role": "assistant",
        "content": response,
        "timestamp": datetime.now().isoformat(),
        "generation_time": round(generation_time, 2)
    })
    
    # æ˜¾ç¤ºç”Ÿæˆæ—¶é—´
    st.sidebar.info(f"â±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.1f}ç§’")
    
    # é‡æ–°æ¸²æŸ“é¡µé¢
    st.rerun()

# å¯¹è¯ç»Ÿè®¡
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    st.metric("æ€»å¯¹è¯è½®æ•°", len(st.session_state.messages))
with col2:
    user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
    st.metric("ç”¨æˆ·æé—®", user_msgs)

# å…è´£å£°æ˜
st.markdown("""
<div class="disclaimer-box">
    <strong>âš ï¸ é‡è¦åŒ»ç–—å…è´£å£°æ˜ï¼š</strong><br>
    1. æœ¬åŠ©æ‰‹ç”±äººå·¥æ™ºèƒ½é©±åŠ¨ï¼Œæä¾›çš„ä¿¡æ¯ä»…ä¾›å‚è€ƒå’Œæ•™è‚²ç›®çš„<br>
    2. <strong>ä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€è¯Šæ–­æˆ–æ²»ç–—</strong><br>
    3. å¦‚æœ‰ç´§æ€¥åŒ»ç–—æƒ…å†µï¼Œè¯·ç«‹å³è”ç³»å½“åœ°æ€¥æ•‘æœåŠ¡æˆ–å‰å¾€æœ€è¿‘åŒ»é™¢<br>
    4. åœ¨ä½¿ç”¨ä»»ä½•è¯ç‰©æˆ–æ²»ç–—æ–¹æ¡ˆå‰ï¼Œå¿…é¡»å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ<br>
    5. æ¨¡å‹ç”Ÿæˆå†…å®¹å¯èƒ½å­˜åœ¨ä¸å‡†ç¡®æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Œè¯·è°¨æ…å‚è€ƒ<br>
    6. å¯¹äºå› ä½¿ç”¨æœ¬åŠ©æ‰‹æä¾›çš„ä¿¡æ¯è€Œå¯¼è‡´çš„ä»»ä½•åæœï¼Œå¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»
</div>
""", unsafe_allow_html=True)

# é¡µè„š
st.markdown("---")
st.caption(f"""
<div style="text-align: center; color: #757575;">
    åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ v1.0 | åŸºäº Qwen2.5-7B-Instruct å¾®è°ƒ | {datetime.now().year}
</div>
""", unsafe_allow_html=True)

# è¿è¡Œæç¤º
st.markdown("""
<script>
    // è‡ªåŠ¨èšç„¦åˆ°è¾“å…¥æ¡†
    setTimeout(function() {
        var textarea = document.querySelector('textarea');
        if (textarea) {
            textarea.focus();
        }
    }, 500);
</script>
""", unsafe_allow_html=True)